{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c2e64cb"
      },
      "source": [
        "# Task\n",
        "Develop a hand gesture recognition model that can accurately identify and classify different hand gestures from image or video data, enabling intuitive human-computer interaction and gesture-based control systems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccd362ab"
      },
      "source": [
        "## Data collection and preparation\n",
        "\n",
        "### Subtask:\n",
        "Gather a dataset of hand gesture images or videos. Preprocess the data by resizing, normalizing, and augmenting it.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6868cde9"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to gather a dataset of hand gesture images and preprocess them. Since I don't have a specific dataset provided, I'll assume a directory structure where each subdirectory represents a gesture class and contains images for that class. I will then load these images, resize them, and normalize their pixel values. I will also include a placeholder for data augmentation, although I won't implement specific augmentation techniques in this initial step. Finally, I'll store the preprocessed images and their labels as NumPy arrays.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "8ca550e2",
        "outputId": "5dc3b9e3-2be6-4812-e0b6-e27d8fc118ac"
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define the path to the dataset directory\n",
        "# Replace this with the actual path to your dataset\n",
        "dataset_dir = 'hand_gesture_dataset'\n",
        "\n",
        "# Define the target image dimensions\n",
        "img_width, img_height = 224, 224\n",
        "\n",
        "# Initialize lists to store preprocessed data and labels\n",
        "data = []\n",
        "labels = []\n",
        "class_names = sorted(os.listdir(dataset_dir))\n",
        "\n",
        "# Iterate through the dataset directory\n",
        "for class_name in class_names:\n",
        "    class_dir = os.path.join(dataset_dir, class_name)\n",
        "    if os.path.isdir(class_dir):\n",
        "        for image_name in os.listdir(class_dir):\n",
        "            image_path = os.path.join(class_dir, image_name)\n",
        "            # Read the image\n",
        "            image = cv2.imread(image_path)\n",
        "            if image is not None:\n",
        "                # Resize the image\n",
        "                image = cv2.resize(image, (img_width, img_height))\n",
        "                # Normalize pixel values to [0, 1]\n",
        "                image = image.astype('float32') / 255.0\n",
        "                # Append the preprocessed image and label\n",
        "                data.append(image)\n",
        "                labels.append(class_names.index(class_name))\n",
        "\n",
        "# Convert lists to NumPy arrays\n",
        "data = np.array(data)\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Print the shapes of the resulting arrays\n",
        "print(\"Shape of data array:\", data.shape)\n",
        "print(\"Shape of labels array:\", labels.shape)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'hand_gesture_dataset'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2509275320.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mclass_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Iterate through the dataset directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'hand_gesture_dataset'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed6fe3ad"
      },
      "source": [
        "## Data collection and preparation\n",
        "\n",
        "### Subtask:\n",
        "Gather a dataset of hand gesture images or videos. Preprocess the data by resizing, normalizing, and augmenting it.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87894cf0"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt failed because the dataset directory was not found. I need to ensure the directory exists before trying to read from it. Since I cannot create directories or download data, I will assume the user will provide the correct path to the dataset directory and modify the code to handle potential issues with file reading more gracefully. I will also add the data augmentation and train/test split steps as outlined in the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf2be02c",
        "outputId": "a3e64493-9694-4d14-f6a5-a12ccbef6885"
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "\n",
        "# Define the path to the dataset directory\n",
        "# Replace this with the actual path to your dataset\n",
        "dataset_dir = 'hand_gesture_dataset'\n",
        "\n",
        "# Define the target image dimensions\n",
        "img_width, img_height = 224, 224\n",
        "\n",
        "# Initialize lists to store preprocessed data and labels\n",
        "data = []\n",
        "labels = []\n",
        "class_names = []\n",
        "\n",
        "# Check if the dataset directory exists\n",
        "if not os.path.isdir(dataset_dir):\n",
        "    print(f\"Error: Dataset directory not found at {dataset_dir}\")\n",
        "else:\n",
        "    class_names = sorted(os.listdir(dataset_dir))\n",
        "    # Iterate through the dataset directory\n",
        "    for class_name in class_names:\n",
        "        class_dir = os.path.join(dataset_dir, class_name)\n",
        "        if os.path.isdir(class_dir):\n",
        "            for image_name in os.listdir(class_dir):\n",
        "                image_path = os.path.join(class_dir, image_name)\n",
        "                # Read the image\n",
        "                image = cv2.imread(image_path)\n",
        "                if image is not None:\n",
        "                    # Resize the image\n",
        "                    image = cv2.resize(image, (img_width, img_height))\n",
        "                    # Normalize pixel values to [0, 1]\n",
        "                    image = image.astype('float32') / 255.0\n",
        "                    # Append the preprocessed image and label\n",
        "                    data.append(image)\n",
        "                    labels.append(class_names.index(class_name))\n",
        "                else:\n",
        "                    print(f\"Warning: Could not read image file: {image_path}\")\n",
        "        else:\n",
        "            print(f\"Warning: Skipping non-directory entry in dataset_dir: {class_dir}\")\n",
        "\n",
        "\n",
        "# Convert lists to NumPy arrays\n",
        "data = np.array(data)\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Implement data augmentation (using TensorFlow's ImageDataGenerator for simplicity)\n",
        "# This part assumes you have a sufficient amount of data loaded.\n",
        "# If data is empty, this part will be skipped.\n",
        "if data.shape[0] > 0:\n",
        "    datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "        rotation_range=20,\n",
        "        zoom_range=0.15,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        shear_range=0.15,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode=\"nearest\"\n",
        "    )\n",
        "    augmented_labels = []\n",
        "    for i in range(data.shape[0]):\n",
        "         img = data[i]\n",
        "         img = np.expand_dims(img, 0) # Add batch dimension\n",
        "         gen_aug = datagen.flow(img, batch_size=1)\n",
        "         for _ in range(5): # Generate 5 augmented images per original image\n",
        "             aug_img = next(gen_aug)[0]\n",
        "             augmented_data.append(aug_img)\n",
        "             augmented_labels.append(labels[i])\n",
        "\n",
        "    augmented_data = np.array(augmented_data)\n",
        "    augmented_labels = np.array(augmented_labels)\n",
        "    print(\"Shape of augmented data array (demonstration):\", augmented_data.shape)\n",
        "\n",
        "\n",
        "    # Split the preprocessed data into training and testing sets\n",
        "    train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=0.2, random_state=42, stratify=labels)\n",
        "\n",
        "    print(\"Shape of training data array:\", train_data.shape)\n",
        "    print(\"Shape of testing data array:\", test_data.shape)\n",
        "    print(\"Shape of training labels array:\", train_labels.shape)\n",
        "    print(\"Shape of testing labels array:\", test_labels.shape)\n",
        "    print(\"Number of classes:\", len(class_names))\n",
        "    print(\"Class names:\", class_names)\n",
        "\n",
        "else:\n",
        "    print(\"No data loaded. Cannot perform train/test split or augmentation.\")\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Dataset directory not found at hand_gesture_dataset\n",
            "No data loaded. Cannot perform train/test split or augmentation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76f4203c"
      },
      "source": [
        "## Model selection\n",
        "\n",
        "### Subtask:\n",
        "Choose a suitable deep learning architecture for gesture recognition, such as a Convolutional Neural Network (CNN) or a Recurrent Neural Network (RNN) for video data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d923c946"
      },
      "source": [
        "## Model selection\n",
        "\n",
        "### Subtask:\n",
        "Choose a suitable deep learning architecture for gesture recognition, such as a Convolutional Neural Network (CNN) or a Recurrent Neural Network (RNN) for video data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c22385d8"
      },
      "source": [
        "**Reasoning**:\n",
        "Based on the instructions, I need to state the chosen architecture (CNN) and explain why it's suitable for image data, assuming the dataset contains image data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "393abfa4",
        "outputId": "46c00d82-2009-4963-9810-4087e3b015fd"
      },
      "source": [
        "# Based on the assumption that the dataset contains image data, a Convolutional Neural Network (CNN)\n",
        "# is a suitable deep learning architecture for hand gesture recognition.\n",
        "\n",
        "# Reasoning for choosing CNN:\n",
        "# CNNs are particularly effective for image data because they can automatically learn spatial\n",
        "# hierarchies of features. They use convolutional layers to detect patterns such as edges,\n",
        "# corners, and textures in the input images. Pooling layers help to reduce the spatial\n",
        "# dimensions of the feature maps, making the model more robust to variations in the position\n",
        "# of features. These learned features are then passed through fully connected layers for\n",
        "# classification. This architecture is well-suited for tasks like image-based gesture\n",
        "# recognition where recognizing visual patterns is crucial.\n",
        "\n",
        "print(\"Chosen deep learning architecture: Convolutional Neural Network (CNN)\")\n",
        "print(\"Reasoning: CNNs are highly effective for image data as they automatically learn spatial hierarchies of features through convolutional and pooling layers, which is crucial for recognizing visual patterns in hand gestures.\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chosen deep learning architecture: Convolutional Neural Network (CNN)\n",
            "Reasoning: CNNs are highly effective for image data as they automatically learn spatial hierarchies of features through convolutional and pooling layers, which is crucial for recognizing visual patterns in hand gestures.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "677dc3ad"
      },
      "source": [
        "## Model implementation\n",
        "\n",
        "### Subtask:\n",
        "Implement the chosen model architecture using a deep learning framework like TensorFlow or PyTorch.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66250b5c"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the necessary Keras modules and define the CNN model architecture as described in the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "8e64320d",
        "outputId": "708ae759-f267-415f-b262-52442174929e"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "# Assume number of classes is known, e.g., from previous data loading step\n",
        "# Since the previous data loading step failed, we'll use a placeholder\n",
        "num_classes = 10 # Placeholder for the number of hand gesture classes\n",
        "\n",
        "# Define the sequential CNN model\n",
        "model = Sequential()\n",
        "\n",
        "# Add convolutional and pooling layers\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(img_height, img_width, 3)))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "# Add Flatten layer\n",
        "model.add(Flatten())\n",
        "\n",
        "# Add Dense layers for classification\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(num_classes, activation='softmax')) # Output layer with softmax for multi-class classification\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m222\u001b[0m, \u001b[38;5;34m222\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m111\u001b[0m, \u001b[38;5;34m111\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m109\u001b[0m, \u001b[38;5;34m109\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m, \u001b[38;5;34m52\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m86528\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │    \u001b[38;5;34m11,075,712\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,290\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">222</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">222</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">111</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">111</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">109</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">109</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">86528</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │    <span style=\"color: #00af00; text-decoration-color: #00af00\">11,075,712</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m11,170,250\u001b[0m (42.61 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,170,250</span> (42.61 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m11,170,250\u001b[0m (42.61 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,170,250</span> (42.61 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2a8a778"
      },
      "source": [
        "## Model training\n",
        "\n",
        "### Subtask:\n",
        "Train the model on the prepared dataset, monitoring its performance during training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2a69942"
      },
      "source": [
        "**Reasoning**:\n",
        "Compile and train the implemented CNN model as instructed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "3392eeaa",
        "outputId": "747172e6-4df7-4d3e-af44-9b689a22793b"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# 1. Compile the implemented CNN model.\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Convert labels to one-hot encoding if they are not already\n",
        "# Assuming train_labels is a numpy array of integer labels\n",
        "if len(train_labels.shape) == 1:\n",
        "    train_labels_one_hot = tf.keras.utils.to_categorical(train_labels, num_classes=num_classes)\n",
        "else:\n",
        "    train_labels_one_hot = train_labels\n",
        "\n",
        "\n",
        "# 2. Train the compiled model.\n",
        "# Use the train_data and train_labels (one-hot encoded).\n",
        "# Use a batch size of 32 and train for 10 epochs.\n",
        "# Store the training history in a variable.\n",
        "history = model.fit(train_data, train_labels_one_hot,\n",
        "                    batch_size=32,\n",
        "                    epochs=10)\n",
        "\n",
        "# 3. Monitor the training process by observing the loss and accuracy for each epoch.\n",
        "# The model.fit method automatically prints the loss and accuracy for each epoch.\n",
        "\n",
        "# 4. Print a message indicating that the training is complete.\n",
        "print(\"Model training complete.\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_labels' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1286105798.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Convert labels to one-hot encoding if they are not already\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Assuming train_labels is a numpy array of integer labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mtrain_labels_one_hot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_labels' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7ec7335"
      },
      "source": [
        "## Model training\n",
        "\n",
        "### Subtask:\n",
        "Retry training the model on the prepared dataset, monitoring its performance during training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e698690"
      },
      "source": [
        "**Reasoning**:\n",
        "Compile and train the model using the specified optimizer, loss function, metrics, batch size, and epochs, ensuring the labels are one-hot encoded.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4385c297",
        "outputId": "c8c9a994-2544-4ed1-b3be-8bb7aaab4d5c"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# 1. Compile the previously implemented CNN model.\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# 2. Ensure that the train_data and train_labels variables are available and train_labels is one-hot encoded.\n",
        "# Assuming train_data and train_labels exist from a previous successful data preparation step.\n",
        "# If train_labels is not in one-hot encoded format, convert it.\n",
        "if 'train_labels' in locals() and train_labels is not None and 'train_data' in locals() and train_data is not None:\n",
        "    if len(train_labels.shape) == 1:\n",
        "        # Assuming num_classes is available from previous steps\n",
        "        train_labels_one_hot = tf.keras.utils.to_categorical(train_labels, num_classes=num_classes)\n",
        "        print(\"Converted train_labels to one-hot encoding.\")\n",
        "    else:\n",
        "        train_labels_one_hot = train_labels\n",
        "        print(\"train_labels is already one-hot encoded.\")\n",
        "\n",
        "    # 3. Train the compiled model.\n",
        "    # Use the train_data and the prepared train_labels (one-hot encoded).\n",
        "    # Use a batch size of 32 and train for 10 epochs.\n",
        "    # Store the training history in a variable.\n",
        "    print(\"Starting model training...\")\n",
        "    history = model.fit(train_data, train_labels_one_hot,\n",
        "                        batch_size=32,\n",
        "                        epochs=10)\n",
        "\n",
        "    # 6. Print a message indicating that the training is complete.\n",
        "    print(\"Model training complete.\")\n",
        "\n",
        "else:\n",
        "    print(\"Error: train_data or train_labels not found. Please ensure data preparation was successful.\")\n",
        "    history = None # Set history to None if training could not start"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: train_data or train_labels not found. Please ensure data preparation was successful.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c486fed"
      },
      "source": [
        "## Model evaluation\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the trained model's accuracy and performance on a separate test set.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e35ce873"
      },
      "source": [
        "**Reasoning**:\n",
        "Evaluate the trained model using the test data and labels.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a33adfc7",
        "outputId": "3d1bb992-6e5b-42a6-ab17-5fc07945c582"
      },
      "source": [
        "# Evaluate the trained model using the test data and labels.\n",
        "# Assuming 'model', 'test_data', and 'test_labels' are available from previous steps.\n",
        "\n",
        "# Ensure test_labels is in the correct format (one-hot encoded if needed by the loss function)\n",
        "# Based on the previous compilation using 'categorical_crossentropy', the model expects\n",
        "# one-hot encoded labels for the output layer with 'softmax' activation.\n",
        "# Assuming test_labels is a numpy array of integer labels.\n",
        "if 'test_labels' in locals() and test_labels is not None:\n",
        "    if len(test_labels.shape) == 1:\n",
        "        # Assuming num_classes is available from previous steps\n",
        "        test_labels_one_hot = tf.keras.utils.to_categorical(test_labels, num_classes=num_classes)\n",
        "        print(\"Converted test_labels to one-hot encoding for evaluation.\")\n",
        "    else:\n",
        "        test_labels_one_hot = test_labels\n",
        "        print(\"test_labels is already one-hot encoded.\")\n",
        "\n",
        "    # Evaluate the model\n",
        "    print(\"Evaluating the model on the test set...\")\n",
        "    evaluation_results = model.evaluate(test_data, test_labels_one_hot, verbose=2)\n",
        "\n",
        "    # Print the evaluation results\n",
        "    print(\"\\nModel Evaluation Results:\")\n",
        "    # The .evaluate() method returns a list of metrics, where the first element is the loss\n",
        "    # and the subsequent elements are the metrics defined during model compilation (e.g., accuracy).\n",
        "    metric_names = model.metrics_names\n",
        "    for name, value in zip(metric_names, evaluation_results):\n",
        "        print(f\"{name}: {value:.4f}\")\n",
        "\n",
        "else:\n",
        "    print(\"Error: test_data or test_labels not found. Cannot perform evaluation.\")\n",
        "    evaluation_results = None # Set evaluation_results to None if evaluation could not start\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: test_data or test_labels not found. Cannot perform evaluation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "568870ff"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The initial attempts to load and preprocess the hand gesture dataset failed because the specified directory (`hand_gesture_dataset`) was not found.\n",
        "*   Due to the failure in data loading, subsequent steps such as splitting data into training and testing sets, training the model, and evaluating the model could not be completed as the necessary variables (`train_data`, `train_labels`, `test_data`, `test_labels`) were unavailable.\n",
        "*   A Convolutional Neural Network (CNN) was chosen as the suitable architecture for image-based hand gesture recognition, and its structure was successfully implemented using TensorFlow/Keras.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The most critical next step is to ensure the `hand_gesture_dataset` directory is correctly placed or the path is updated to where the dataset resides, allowing the data loading and preprocessing steps to succeed.\n",
        "*   Once the data is successfully loaded and split, the model training and evaluation steps can be executed to assess the performance of the implemented CNN architecture.\n"
      ]
    }
  ]
}